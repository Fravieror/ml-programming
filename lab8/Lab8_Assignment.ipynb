{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lab8 Assignment Task PROG8245 - NLP Introduction"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Name: Francisco Javier Diaz Roa\n",
    "### ID: 8975938"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using the discussed topic in class for tokenizers, stop-word removal, stemming/lemmatization, and POS Tagging. <br><br>\n",
    "Create **ONE** function, that takes as an input a string, and returns the output of a string after stemming/lemmatization.<br><br>\n",
    "**Kindly note that you are required to consider the POS Tag while doing your stemming or lemmatization step (you should use whatever is more suitable for this task)** <br><br>\n",
    "After creating the function, you need to run your function on 10 **Random** files from reuters corpus, an example of how to download and load a file of reuters corpus is below. <br><br>\n",
    "**Your 10 **Random** files should be retrieved by getting a random array of length 10 which picks numbers RANDOMLY from 0 to len(reuters.fileids()), then the elements retrieved will be your corpus.<br> <br>*You need to set your Seed to be Equal to the last 3 digits in your studentID.*<br>**\n",
    "**You may need to tailor your task based on the dataset to remove some special characters.**\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Error loading punkt: <urlopen error [SSL:\n",
      "[nltk_data]     CERTIFICATE_VERIFY_FAILED] certificate verify failed:\n",
      "[nltk_data]     unable to get local issuer certificate (_ssl.c:1000)>\n",
      "[nltk_data] Error loading stopwords: <urlopen error [SSL:\n",
      "[nltk_data]     CERTIFICATE_VERIFY_FAILED] certificate verify failed:\n",
      "[nltk_data]     unable to get local issuer certificate (_ssl.c:1000)>\n"
     ]
    },
    {
     "ename": "OSError",
     "evalue": "[E050] Can't find model 'en_core_web_sm'. It doesn't seem to be a Python package or a valid path to a data directory.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 12\u001b[0m\n\u001b[1;32m     10\u001b[0m nltk\u001b[38;5;241m.\u001b[39mdownload(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpunkt\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m     11\u001b[0m nltk\u001b[38;5;241m.\u001b[39mdownload(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mstopwords\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m---> 12\u001b[0m nlp \u001b[38;5;241m=\u001b[39m \u001b[43mspacy\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43men_core_web_sm\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     14\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mpreprocess_text\u001b[39m(text):\n\u001b[1;32m     15\u001b[0m      \u001b[38;5;66;03m# Tokenization\u001b[39;00m\n\u001b[1;32m     16\u001b[0m     tokens \u001b[38;5;241m=\u001b[39m word_tokenize(text)\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/spacy/__init__.py:51\u001b[0m, in \u001b[0;36mload\u001b[0;34m(name, vocab, disable, enable, exclude, config)\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mload\u001b[39m(\n\u001b[1;32m     28\u001b[0m     name: Union[\u001b[38;5;28mstr\u001b[39m, Path],\n\u001b[1;32m     29\u001b[0m     \u001b[38;5;241m*\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     34\u001b[0m     config: Union[Dict[\u001b[38;5;28mstr\u001b[39m, Any], Config] \u001b[38;5;241m=\u001b[39m util\u001b[38;5;241m.\u001b[39mSimpleFrozenDict(),\n\u001b[1;32m     35\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Language:\n\u001b[1;32m     36\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Load a spaCy model from an installed package or a local path.\u001b[39;00m\n\u001b[1;32m     37\u001b[0m \n\u001b[1;32m     38\u001b[0m \u001b[38;5;124;03m    name (str): Package name or model path.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     49\u001b[0m \u001b[38;5;124;03m    RETURNS (Language): The loaded nlp object.\u001b[39;00m\n\u001b[1;32m     50\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m---> 51\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mutil\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload_model\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     52\u001b[0m \u001b[43m        \u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     53\u001b[0m \u001b[43m        \u001b[49m\u001b[43mvocab\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mvocab\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     54\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdisable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdisable\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     55\u001b[0m \u001b[43m        \u001b[49m\u001b[43menable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43menable\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     56\u001b[0m \u001b[43m        \u001b[49m\u001b[43mexclude\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mexclude\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     57\u001b[0m \u001b[43m        \u001b[49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     58\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/spacy/util.py:472\u001b[0m, in \u001b[0;36mload_model\u001b[0;34m(name, vocab, disable, enable, exclude, config)\u001b[0m\n\u001b[1;32m    470\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01min\u001b[39;00m OLD_MODEL_SHORTCUTS:\n\u001b[1;32m    471\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mIOError\u001b[39;00m(Errors\u001b[38;5;241m.\u001b[39mE941\u001b[38;5;241m.\u001b[39mformat(name\u001b[38;5;241m=\u001b[39mname, full\u001b[38;5;241m=\u001b[39mOLD_MODEL_SHORTCUTS[name]))  \u001b[38;5;66;03m# type: ignore[index]\u001b[39;00m\n\u001b[0;32m--> 472\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mIOError\u001b[39;00m(Errors\u001b[38;5;241m.\u001b[39mE050\u001b[38;5;241m.\u001b[39mformat(name\u001b[38;5;241m=\u001b[39mname))\n",
      "\u001b[0;31mOSError\u001b[0m: [E050] Can't find model 'en_core_web_sm'. It doesn't seem to be a Python package or a valid path to a data directory."
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "import spacy\n",
    "import random\n",
    "import json\n",
    "from nltk.corpus import reuters\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "# Download the required resources\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "def preprocess_text(text):\n",
    "  # Tokenization\n",
    "  # what is word_tokenize()?\n",
    "    # It is a function in the nltk library that tokenizes a text into words.\n",
    "    # what does it mean to tokenize a text?\n",
    "    # Tokenization is the process of converting a text into smaller parts called tokens.\n",
    "    # small parts can be words, sentences, or phrases.\n",
    "    # depending on the type of tokenization. In this case, it is word tokenization.\n",
    "    # in other words, it is converting a text into a list of words.\n",
    "    tokens = word_tokenize(text)\n",
    "    \n",
    "    # Remove special characters\n",
    "    # what is word.isalnum()?\n",
    "    # Return True if all characters in the word are alphanumeric and there is at least one character, False otherwise.\n",
    "    # So what is the purpose of this line?\n",
    "    # To remove special characters from the tokens. if the token is not alphanumeric, it will be removed.\n",
    "    tokens = [word for word in tokens if word.isalnum()]\n",
    "    \n",
    "    # Stop-words removal\n",
    "    # what is stopwords.words('english')?\n",
    "    # It is a list of stopwords in the English language.\n",
    "    # Stopwords are common words that do not contribute much to the meaning of a text.\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    # what is word.lower()?\n",
    "    # It is a function that converts a word to lowercase.\n",
    "    # what is the purpose of this line?\n",
    "    # To remove stopwords from the tokens. if the lowercase version of the token is in the stop_words set, it will be removed.\n",
    "    filtered_tokens = [word for word in tokens if word.lower() not in stop_words]\n",
    "    \n",
    "    # POS tagging and lemmatization using spaCy\n",
    "    # what is nlp()?\n",
    "    # It is a function in the spaCy library that processes a text and returns a Doc object.\n",
    "    # in other words, it is used to process the text using spaCy. to get the tokens, POS tags, lemmas, etc.\n",
    "    # what is token.lemma_?\n",
    "    # It is the lemma of a token. A lemma is the base form of a word.\n",
    "    # For example, the lemma of \"running\" is \"run\".\n",
    "    doc = nlp(\" \".join(filtered_tokens))\n",
    "    lemmatized_tokens = [token.lemma_ for token in doc]\n",
    "    \n",
    "    return lemmatized_tokens\n",
    "\n",
    "# Example usage\n",
    "text = \"There was an earthquake near D.C. Iâ€™ve even felt it in Philadelphia, New York, etc.\"\n",
    "preprocessed_text = preprocess_text(text)\n",
    "print(preprocessed_text)\n",
    "\n",
    "\n",
    "# Download the required resources\n",
    "nltk.download('reuters')\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "# Set the seed\n",
    "random.seed(938)\n",
    "\n",
    "# Get 10 random fileids\n",
    "fileids = reuters.fileids()\n",
    "random_fileids = random.sample(fileids, 10)\n",
    "\n",
    "# Process and store the output for the 10 random files in a dictionary\n",
    "preprocessed_data = {}\n",
    "for fileid in random_fileids:\n",
    "    text = reuters.raw(fileid)\n",
    "    preprocessed_text = preprocess_text(text)\n",
    "    preprocessed_data[fileid] = preprocessed_text\n",
    "\n",
    "# Save the dictionary as a JSON file\n",
    "with open('preprocessed_reuters.json', 'w') as json_file:\n",
    "    json.dump(preprocessed_data, json_file, indent=4)\n",
    "\n",
    "print(\"Preprocessed data has been saved to 'preprocessed_reuters.json'\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Last Step\n",
    "After finishing your code, run your code and save the result in a python dictionary, which would be of format:<br>\n",
    "{DocumentID: [List of Words], <br>\n",
    "...} <br>\n",
    "Save your python dictionary as a JSON file, or Pickle file. <br>\n",
    "A sample code for saving a python dictionary is available at the end of this notebook.<br>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package reuters to\n",
      "[nltk_data]     C:\\Users\\Islam\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package reuters is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "10788"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.corpus import reuters\n",
    "nltk.download('reuters') #downloading reuters corpus\n",
    "len(reuters.fileids()) #checking how many files are there"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'ASIAN EXPORTERS FEAR DAMAGE FROM U.S.-JAPAN RIFT\\n  Mounting trade friction between the\\n  U.S. And Japan has raised fears among many of Asia\\'s exporting\\n  nations that the row could inflict far-reaching economic\\n  damage, businessmen and officials said.\\n      They told Reuter correspondents in Asian capitals a U.S.\\n  Move against Japan might boost protectionist sentiment in the\\n  U.S. And lead to curbs on American imports of their products.\\n      But some exporters said that while the conflict would hurt\\n  them in the long-run, in the short-term Tokyo\\'s loss might be\\n  their gain.\\n      The U.S. Has said it will impose 300 mln dlrs of tariffs on\\n  imports of Japanese electronics goods on April 17, in\\n  retaliation for Japan\\'s alleged failure to stick to a pact not\\n  to sell semiconductors on world markets at below cost.\\n      Unofficial Japanese estimates put the impact of the tariffs\\n  at 10 billion dlrs and spokesmen for major electronics firms\\n  said they would virtually halt exports of products hit by the\\n  new taxes.\\n      \"We wouldn\\'t be able to do business,\" said a spokesman for\\n  leading Japanese electronics firm Matsushita Electric\\n  Industrial Co Ltd &lt;MC.T>.\\n      \"If the tariffs remain in place for any length of time\\n  beyond a few months it will mean the complete erosion of\\n  exports (of goods subject to tariffs) to the U.S.,\" said Tom\\n  Murtha, a stock analyst at the Tokyo office of broker &lt;James\\n  Capel and Co>.\\n      In Taiwan, businessmen and officials are also worried.\\n      \"We are aware of the seriousness of the U.S. Threat against\\n  Japan because it serves as a warning to us,\" said a senior\\n  Taiwanese trade official who asked not to be named.\\n      Taiwan had a trade trade surplus of 15.6 billion dlrs last\\n  year, 95 pct of it with the U.S.\\n      The surplus helped swell Taiwan\\'s foreign exchange reserves\\n  to 53 billion dlrs, among the world\\'s largest.\\n      \"We must quickly open our markets, remove trade barriers and\\n  cut import tariffs to allow imports of U.S. Products, if we\\n  want to defuse problems from possible U.S. Retaliation,\" said\\n  Paul Sheen, chairman of textile exporters &lt;Taiwan Safe Group>.\\n      A senior official of South Korea\\'s trade promotion\\n  association said the trade dispute between the U.S. And Japan\\n  might also lead to pressure on South Korea, whose chief exports\\n  are similar to those of Japan.\\n      Last year South Korea had a trade surplus of 7.1 billion\\n  dlrs with the U.S., Up from 4.9 billion dlrs in 1985.\\n      In Malaysia, trade officers and businessmen said tough\\n  curbs against Japan might allow hard-hit producers of\\n  semiconductors in third countries to expand their sales to the\\n  U.S.\\n      In Hong Kong, where newspapers have alleged Japan has been\\n  selling below-cost semiconductors, some electronics\\n  manufacturers share that view. But other businessmen said such\\n  a short-term commercial advantage would be outweighed by\\n  further U.S. Pressure to block imports.\\n      \"That is a very short-term view,\" said Lawrence Mills,\\n  director-general of the Federation of Hong Kong Industry.\\n      \"If the whole purpose is to prevent imports, one day it will\\n  be extended to other sources. Much more serious for Hong Kong\\n  is the disadvantage of action restraining trade,\" he said.\\n      The U.S. Last year was Hong Kong\\'s biggest export market,\\n  accounting for over 30 pct of domestically produced exports.\\n      The Australian government is awaiting the outcome of trade\\n  talks between the U.S. And Japan with interest and concern,\\n  Industry Minister John Button said in Canberra last Friday.\\n      \"This kind of deterioration in trade relations between two\\n  countries which are major trading partners of ours is a very\\n  serious matter,\" Button said.\\n      He said Australia\\'s concerns centred on coal and beef,\\n  Australia\\'s two largest exports to Japan and also significant\\n  U.S. Exports to that country.\\n      Meanwhile U.S.-Japanese diplomatic manoeuvres to solve the\\n  trade stand-off continue.\\n      Japan\\'s ruling Liberal Democratic Party yesterday outlined\\n  a package of economic measures to boost the Japanese economy.\\n      The measures proposed include a large supplementary budget\\n  and record public works spending in the first half of the\\n  financial year.\\n      They also call for stepped-up spending as an emergency\\n  measure to stimulate the economy despite Prime Minister\\n  Yasuhiro Nakasone\\'s avowed fiscal reform program.\\n      Deputy U.S. Trade Representative Michael Smith and Makoto\\n  Kuroda, Japan\\'s deputy minister of International Trade and\\n  Industry (MITI), are due to meet in Washington this week in an\\n  effort to end the dispute.\\n  \\n\\n'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# to read specific file\n",
    "document_id = 'training/9865'\n",
    "text = reuters.raw(document_id) #reading a sample file from reuters\n",
    "text "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Saving python dictionary:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "my_dict = {\"training/9865\": ['fear','trade', 'raise','Asia', 'export']}\n",
    "# Save the dictionary to a file using Pickle\n",
    "with open('output.pkl', 'wb') as file:\n",
    "    pickle.dump(my_dict, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'training/9865': ['fear', 'trade', 'raise', 'Asia', 'export']}\n"
     ]
    }
   ],
   "source": [
    "# to read the pkl file:\n",
    "\n",
    "with open('output.pkl', 'rb') as file:\n",
    "    loaded_dict = pickle.load(file)\n",
    "    \n",
    "# Print the dictionary\n",
    "print(loaded_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
