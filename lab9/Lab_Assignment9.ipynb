{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lab Assignment 9\n",
    "#### Student Name:Francisco Javier Diaz Roa\n",
    "#### ID:8975938"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_sentences = [\n",
    "    \"Python is a versatile programming language.\",\n",
    "    \"JavaScript is widely used for web development.\",\n",
    "    \"Java is known for its platform independence.\",\n",
    "    \"Programming involves writing code to solve problems.\",\n",
    "    \"Data structures are crucial for efficient programming.\",\n",
    "    \"Algorithms are step-by-step instructions for solving problems.\",\n",
    "    \"Version control systems help manage code changes in collaboration.\",\n",
    "    \"Debugging is the process of finding and fixing errors in code.\",\n",
    "    \"Web frameworks simplify the development of web applications.\",\n",
    "    \"Artificial intelligence can be applied in various programming tasks.\"\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Given the sample sentences above. You are required for this assignment to implement four functions **from scratch**. <br>\n",
    "You are required to preprocess the text and apply the tokenization process as done in assignment 8. (3)\n",
    "***THEN***\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentence 1: ['python', 'is', 'a', 'versatile', 'programming', 'language']\n",
      "Sentence 2: ['javascript', 'is', 'widely', 'used', 'for', 'web', 'development']\n",
      "Sentence 3: ['java', 'is', 'known', 'for', 'its', 'platform', 'independence']\n",
      "Sentence 4: ['programming', 'involves', 'writing', 'code', 'to', 'solve', 'problems']\n",
      "Sentence 5: ['data', 'structures', 'are', 'crucial', 'for', 'efficient', 'programming']\n",
      "Sentence 6: ['algorithms', 'are', 'stepbystep', 'instructions', 'for', 'solving', 'problems']\n",
      "Sentence 7: ['version', 'control', 'systems', 'help', 'manage', 'code', 'changes', 'in', 'collaboration']\n",
      "Sentence 8: ['debugging', 'is', 'the', 'process', 'of', 'finding', 'and', 'fixing', 'errors', 'in', 'code']\n",
      "Sentence 9: ['web', 'frameworks', 'simplify', 'the', 'development', 'of', 'web', 'applications']\n",
      "Sentence 10: ['artificial', 'intelligence', 'can', 'be', 'applied', 'in', 'various', 'programming', 'tasks']\n"
     ]
    }
   ],
   "source": [
    "import string\n",
    "def convert_to_lowercase(sentences):\n",
    "    \"\"\"Convert all sentences to lowercase.\"\"\"\n",
    "    return [sentence.lower() for sentence in sentences]\n",
    "\n",
    "def remove_punctuation(sentences):\n",
    "    \"\"\"Remove punctuation from sentences.\"\"\"\n",
    "    translator = str.maketrans('', '', string.punctuation)\n",
    "    return [sentence.translate(translator) for sentence in sentences]\n",
    "\n",
    "def tokenize_sentence(sentences):\n",
    "    \"\"\"Split sentences into individual words (tokens).\"\"\"\n",
    "    return [sentence.split() for sentence in sentences]\n",
    "\n",
    "def preprocess_sentences(sentences):\n",
    "    \"\"\"Combine all preprocessing steps: lowercase, remove punctuation, and tokenize.\"\"\"\n",
    "    sentences = convert_to_lowercase(sentences)\n",
    "    sentences = remove_punctuation(sentences)\n",
    "    tokens = tokenize_sentence(sentences)\n",
    "    return tokens\n",
    "\n",
    "# Preprocess the sample sentences\n",
    "preprocessed_tokens = preprocess_sentences(sample_sentences)\n",
    "\n",
    "# Display the preprocessed tokens\n",
    "for i, tokens in enumerate(preprocessed_tokens):\n",
    "    print(f\"Sentence {i+1}: {tokens}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### \n",
    "#### Part 1: Create a method that takes as an input a 2-dimensional list where each of the inner dimensions is a sentence list of tokens, and the outer dimension is the list of the sentences. The method MUST return the inverted index that is sufficient to represent the document. Assume that each sentence is a document and the sentence ID starts from 1. (5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'python': [1]\n",
      "'is': [1, 2, 3, 8]\n",
      "'a': [1]\n",
      "'versatile': [1]\n",
      "'programming': [1, 4, 5, 10]\n",
      "'language': [1]\n",
      "'javascript': [2]\n",
      "'widely': [2]\n",
      "'used': [2]\n",
      "'for': [2, 3, 5, 6]\n",
      "'web': [2, 9, 9]\n",
      "'development': [2, 9]\n",
      "'java': [3]\n",
      "'known': [3]\n",
      "'its': [3]\n",
      "'platform': [3]\n",
      "'independence': [3]\n",
      "'involves': [4]\n",
      "'writing': [4]\n",
      "'code': [4, 7, 8]\n",
      "'to': [4]\n",
      "'solve': [4]\n",
      "'problems': [4, 6]\n",
      "'data': [5]\n",
      "'structures': [5]\n",
      "'are': [5, 6]\n",
      "'crucial': [5]\n",
      "'efficient': [5]\n",
      "'algorithms': [6]\n",
      "'stepbystep': [6]\n",
      "'instructions': [6]\n",
      "'solving': [6]\n",
      "'version': [7]\n",
      "'control': [7]\n",
      "'systems': [7]\n",
      "'help': [7]\n",
      "'manage': [7]\n",
      "'changes': [7]\n",
      "'in': [7, 8, 10]\n",
      "'collaboration': [7]\n",
      "'debugging': [8]\n",
      "'the': [8, 9]\n",
      "'process': [8]\n",
      "'of': [8, 9]\n",
      "'finding': [8]\n",
      "'and': [8]\n",
      "'fixing': [8]\n",
      "'errors': [8]\n",
      "'frameworks': [9]\n",
      "'simplify': [9]\n",
      "'applications': [9]\n",
      "'artificial': [10]\n",
      "'intelligence': [10]\n",
      "'can': [10]\n",
      "'be': [10]\n",
      "'applied': [10]\n",
      "'various': [10]\n",
      "'tasks': [10]\n"
     ]
    }
   ],
   "source": [
    "def get_inverted_index(list_of_sentence_tokens):\n",
    "    \"\"\"\n",
    "    Constructs an inverted index from the list of sentence tokens.\n",
    "    \n",
    "    Parameters:\n",
    "        list_of_sentence_tokens (list of list of str): 2D list where each inner list contains tokens of a sentence.\n",
    "    \n",
    "    Returns:\n",
    "        dict: Inverted index where keys are tokens and values are lists of sentence IDs.\n",
    "    \"\"\"\n",
    "    inverted_index = {}\n",
    "    \n",
    "    for sentence_id, tokens in enumerate(list_of_sentence_tokens, start=1):\n",
    "        for token in tokens:\n",
    "            if token in inverted_index:\n",
    "                inverted_index[token].append(sentence_id)\n",
    "            else:\n",
    "                inverted_index[token] = [sentence_id]\n",
    "    \n",
    "    return inverted_index\n",
    "\n",
    "# Get the inverted index\n",
    "inverted_index = get_inverted_index(preprocessed_tokens)\n",
    "\n",
    "# Display the inverted index\n",
    "for token, sentence_ids in inverted_index.items():\n",
    "    print(f\"'{token}': {sentence_ids}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### \n",
    "#### Part 2: Create a method that takes as an input a 2-dimensional list where each of the inner dimensions is a sentence list of tokens, and the outer dimension is the list of the sentences. The method MUST return the Positional index that is sufficient to represent the document. Assume that each sentence is a document and the sentence ID starts from 1, and the first token in the list is at position 0. Make sure to consider multiple appearance of the same token. (5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'python': [(1, 0)]\n",
      "'is': [(1, 1), (2, 1), (3, 1), (8, 1)]\n",
      "'a': [(1, 2)]\n",
      "'versatile': [(1, 3)]\n",
      "'programming': [(1, 4), (4, 0), (5, 6), (10, 7)]\n",
      "'language': [(1, 5)]\n",
      "'javascript': [(2, 0)]\n",
      "'widely': [(2, 2)]\n",
      "'used': [(2, 3)]\n",
      "'for': [(2, 4), (3, 3), (5, 4), (6, 4)]\n",
      "'web': [(2, 5), (9, 0), (9, 6)]\n",
      "'development': [(2, 6), (9, 4)]\n",
      "'java': [(3, 0)]\n",
      "'known': [(3, 2)]\n",
      "'its': [(3, 4)]\n",
      "'platform': [(3, 5)]\n",
      "'independence': [(3, 6)]\n",
      "'involves': [(4, 1)]\n",
      "'writing': [(4, 2)]\n",
      "'code': [(4, 3), (7, 5), (8, 10)]\n",
      "'to': [(4, 4)]\n",
      "'solve': [(4, 5)]\n",
      "'problems': [(4, 6), (6, 6)]\n",
      "'data': [(5, 0)]\n",
      "'structures': [(5, 1)]\n",
      "'are': [(5, 2), (6, 1)]\n",
      "'crucial': [(5, 3)]\n",
      "'efficient': [(5, 5)]\n",
      "'algorithms': [(6, 0)]\n",
      "'stepbystep': [(6, 2)]\n",
      "'instructions': [(6, 3)]\n",
      "'solving': [(6, 5)]\n",
      "'version': [(7, 0)]\n",
      "'control': [(7, 1)]\n",
      "'systems': [(7, 2)]\n",
      "'help': [(7, 3)]\n",
      "'manage': [(7, 4)]\n",
      "'changes': [(7, 6)]\n",
      "'in': [(7, 7), (8, 9), (10, 5)]\n",
      "'collaboration': [(7, 8)]\n",
      "'debugging': [(8, 0)]\n",
      "'the': [(8, 2), (9, 3)]\n",
      "'process': [(8, 3)]\n",
      "'of': [(8, 4), (9, 5)]\n",
      "'finding': [(8, 5)]\n",
      "'and': [(8, 6)]\n",
      "'fixing': [(8, 7)]\n",
      "'errors': [(8, 8)]\n",
      "'frameworks': [(9, 1)]\n",
      "'simplify': [(9, 2)]\n",
      "'applications': [(9, 7)]\n",
      "'artificial': [(10, 0)]\n",
      "'intelligence': [(10, 1)]\n",
      "'can': [(10, 2)]\n",
      "'be': [(10, 3)]\n",
      "'applied': [(10, 4)]\n",
      "'various': [(10, 6)]\n",
      "'tasks': [(10, 8)]\n"
     ]
    }
   ],
   "source": [
    "def get_positional_index(list_of_sentence_tokens):\n",
    "    \"\"\"\n",
    "    Constructs a positional index from the list of sentence tokens.\n",
    "    \n",
    "    Parameters:\n",
    "        list_of_sentence_tokens (list of list of str): 2D list where each inner list contains tokens of a sentence.\n",
    "    \n",
    "    Returns:\n",
    "        dict: Positional index where keys are tokens and values are lists of tuples (sentence_id, position).\n",
    "    \"\"\"\n",
    "    positional_index = {}\n",
    "    \n",
    "    for sentence_id, tokens in enumerate(list_of_sentence_tokens, start=1):\n",
    "        for position, token in enumerate(tokens):\n",
    "            if token not in positional_index:\n",
    "                positional_index[token] = []\n",
    "            positional_index[token].append((sentence_id, position))\n",
    "    \n",
    "    return positional_index\n",
    "\n",
    "\n",
    "# Get the positional index\n",
    "positional_index = get_positional_index(preprocessed_tokens)\n",
    "\n",
    "# Display the positional index\n",
    "for token, positions in positional_index.items():\n",
    "    print(f\"'{token}': {positions}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### \n",
    "#### Part 3: Create a method that takes as an input a 2-dimensional list where each of the inner dimensions is a sentence list of tokens, and the outer dimension is the list of the sentences. The method MUST return the TF-IDF Matrix that is sufficient to represent the documents. Assume that each sentence is a document and the sentence ID starts from 1. (7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentence 1: defaultdict(<class 'float'>, {'python': 0.26823965207235, 'is': 0.11552453009332421, 'a': 0.26823965207235, 'versatile': 0.26823965207235, 'programming': 0.11552453009332421, 'language': 0.26823965207235})\n",
      "Sentence 2: defaultdict(<class 'float'>, {'javascript': 0.22991970177630003, 'is': 0.09902102579427789, 'widely': 0.22991970177630003, 'used': 0.22991970177630003, 'for': 0.09902102579427789, 'web': 0.17199611490370514, 'development': 0.17199611490370514})\n",
      "Sentence 3: defaultdict(<class 'float'>, {'java': 0.22991970177630003, 'is': 0.09902102579427789, 'known': 0.22991970177630003, 'for': 0.09902102579427789, 'its': 0.22991970177630003, 'platform': 0.22991970177630003, 'independence': 0.22991970177630003})\n",
      "Sentence 4: defaultdict(<class 'float'>, {'programming': 0.09902102579427789, 'involves': 0.22991970177630003, 'writing': 0.22991970177630003, 'code': 0.13089867598202215, 'to': 0.22991970177630003, 'solve': 0.22991970177630003, 'problems': 0.17199611490370514})\n",
      "Sentence 5: defaultdict(<class 'float'>, {'data': 0.22991970177630003, 'structures': 0.22991970177630003, 'are': 0.17199611490370514, 'crucial': 0.22991970177630003, 'for': 0.09902102579427789, 'efficient': 0.22991970177630003, 'programming': 0.09902102579427789})\n",
      "Sentence 6: defaultdict(<class 'float'>, {'algorithms': 0.22991970177630003, 'are': 0.17199611490370514, 'stepbystep': 0.22991970177630003, 'instructions': 0.22991970177630003, 'for': 0.09902102579427789, 'solving': 0.22991970177630003, 'problems': 0.17199611490370514})\n",
      "Sentence 7: defaultdict(<class 'float'>, {'version': 0.17882643471490003, 'control': 0.17882643471490003, 'systems': 0.17882643471490003, 'help': 0.17882643471490003, 'manage': 0.17882643471490003, 'code': 0.10181008131935056, 'changes': 0.17882643471490003, 'in': 0.10181008131935056, 'collaboration': 0.17882643471490003})\n",
      "Sentence 8: defaultdict(<class 'float'>, {'debugging': 0.14631253749400913, 'is': 0.06301338005090412, 'the': 0.10945207312053964, 'process': 0.14631253749400913, 'of': 0.10945207312053964, 'finding': 0.14631253749400913, 'and': 0.14631253749400913, 'fixing': 0.14631253749400913, 'errors': 0.14631253749400913, 'in': 0.08329915744310501, 'code': 0.08329915744310501})\n",
      "Sentence 9: defaultdict(<class 'float'>, {'web': 0.30099320108148403, 'frameworks': 0.20117973905426254, 'simplify': 0.20117973905426254, 'the': 0.15049660054074201, 'development': 0.15049660054074201, 'of': 0.15049660054074201, 'applications': 0.20117973905426254})\n",
      "Sentence 10: defaultdict(<class 'float'>, {'artificial': 0.17882643471490003, 'intelligence': 0.17882643471490003, 'can': 0.17882643471490003, 'be': 0.17882643471490003, 'applied': 0.17882643471490003, 'in': 0.10181008131935056, 'various': 0.17882643471490003, 'programming': 0.07701635339554948, 'tasks': 0.17882643471490003})\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "from collections import defaultdict\n",
    "\n",
    "def compute_tf(sentence_tokens):\n",
    "    \"\"\"Compute term frequency for a list of tokens.\"\"\"\n",
    "    tf_dict = defaultdict(float)\n",
    "    for token in sentence_tokens:\n",
    "        tf_dict[token] += 1.0\n",
    "    # Normalize term frequencies\n",
    "    total_tokens = len(sentence_tokens)\n",
    "    for token in tf_dict:\n",
    "        tf_dict[token] /= total_tokens\n",
    "    return tf_dict\n",
    "\n",
    "def compute_idf(list_of_sentence_tokens):\n",
    "    \"\"\"Compute inverse document frequency for a list of tokenized sentences.\"\"\"\n",
    "    num_sentences = len(list_of_sentence_tokens)\n",
    "    idf_dict = defaultdict(float)\n",
    "    # Count number of sentences containing each token\n",
    "    for sentence_tokens in list_of_sentence_tokens:\n",
    "        unique_tokens = set(sentence_tokens)\n",
    "        for token in unique_tokens:\n",
    "            idf_dict[token] += 1.0\n",
    "    # Compute IDF values\n",
    "    for token in idf_dict:\n",
    "        idf_dict[token] = math.log(num_sentences / (1 + idf_dict[token]))\n",
    "    return idf_dict\n",
    "\n",
    "def compute_tf_idf(tf, idf):\n",
    "    \"\"\"Compute TF-IDF values.\"\"\"\n",
    "    tf_idf_dict = defaultdict(float)\n",
    "    for token in tf:\n",
    "        tf_idf_dict[token] = tf[token] * idf[token]\n",
    "    return tf_idf_dict\n",
    "\n",
    "def get_TFIDF_matrix(list_of_sentence_tokens):\n",
    "    \"\"\"\n",
    "    Compute the TF-IDF matrix for a list of tokenized sentences.\n",
    "    \n",
    "    Parameters:\n",
    "        list_of_sentence_tokens (list of list of str): 2D list where each inner list contains tokens of a sentence.\n",
    "    \n",
    "    Returns:\n",
    "        list of dict: TF-IDF matrix where each entry corresponds to a sentence's TF-IDF values.\n",
    "    \"\"\"\n",
    "    # Step 1: Compute IDF for all tokens\n",
    "    idf = compute_idf(list_of_sentence_tokens)\n",
    "    \n",
    "    # Step 2: Compute TF and TF-IDF for each sentence\n",
    "    tf_idf_matrix = []\n",
    "    for sentence_tokens in list_of_sentence_tokens:\n",
    "        tf = compute_tf(sentence_tokens)\n",
    "        tf_idf = compute_tf_idf(tf, idf)\n",
    "        tf_idf_matrix.append(tf_idf)\n",
    "    \n",
    "    return tf_idf_matrix\n",
    "\n",
    "# Get the TF-IDF matrix\n",
    "tf_idf_matrix = get_TFIDF_matrix(preprocessed_tokens)\n",
    "\n",
    "# Display the TF-IDF matrix\n",
    "for i, tf_idf in enumerate(tf_idf_matrix):\n",
    "    print(f\"Sentence {i+1}: {tf_idf}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### \n",
    "#### Part 4: Create a method that takes as an input: (10)\n",
    " - a 2-dimensional list where each of the inner dimensions is a sentence list of tokens, and the outer dimension is the list of the sentences.\n",
    " - A method name: \"tfidf\", \"inverted\"\n",
    " - A Search Query\n",
    " - Return the rank of the sentences based on the given method and a query <br>\n",
    "\n",
    "***Hint: For inverted index we just want documents that have the query word/words, for tfidf you must show the ranking based on highest tfidf score***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ranked documents (Inverted Index): [1, 4, 5, 10]\n",
      "Ranked documents (TF-IDF): [1, 4, 5, 10, 2, 3, 6, 7, 8, 9]\n"
     ]
    }
   ],
   "source": [
    "def rank_documents_inverted(list_of_sentence_tokens, search_query):\n",
    "    inverted_index = get_inverted_index(list_of_sentence_tokens)\n",
    "    query_tokens = search_query.lower().split()\n",
    "    sentence_scores = defaultdict(int)\n",
    "    \n",
    "    for token in query_tokens:\n",
    "        if token in inverted_index:\n",
    "            for sentence_id in inverted_index[token]:\n",
    "                sentence_scores[sentence_id] += 1\n",
    "    \n",
    "    ranked_sentences = sorted(sentence_scores.keys(), key=lambda x: -sentence_scores[x])\n",
    "    return ranked_sentences\n",
    "\n",
    "def rank_documents_tfidf(list_of_sentence_tokens, search_query):\n",
    "    idf = compute_idf(list_of_sentence_tokens)\n",
    "    query_tokens = search_query.lower().split()\n",
    "    sentence_scores = defaultdict(float)\n",
    "    \n",
    "    for sentence_id, tokens in enumerate(list_of_sentence_tokens, start=1):\n",
    "        tf = compute_tf(tokens)\n",
    "        tf_idf = compute_tf_idf(tf, idf)\n",
    "        score = sum(tf_idf[token] for token in query_tokens if token in tf_idf)\n",
    "        sentence_scores[sentence_id] = score\n",
    "    \n",
    "    ranked_sentences = sorted(sentence_scores.keys(), key=lambda x: -sentence_scores[x])\n",
    "    return ranked_sentences\n",
    "\n",
    "\n",
    "def get_ranked_documents(list_of_sentence_tokens, method_name, search_query):\n",
    "    # TODO: Implement the functionality that returns the rank of the documents based\n",
    "    #  on the method given and the search query\n",
    "    ## If the method is \"inverted\" then rank the documents based on the number of matching tokens \n",
    "    ## If the method is \"tfidf\" then use the tfidf score equation in slides and return ranking based on the score\n",
    "    ## The document with highest relevance should be ranked first\n",
    "    ## list method should return the index of the documents based on highest ranking first\n",
    "    if method_name == \"inverted\":\n",
    "        return rank_documents_inverted(list_of_sentence_tokens, search_query)\n",
    "    elif method_name == \"tfidf\":\n",
    "        return rank_documents_tfidf(list_of_sentence_tokens, search_query)\n",
    "    else:\n",
    "        raise ValueError(\"Invalid method name\")\n",
    "\n",
    "\n",
    "# Get the ranked documents\n",
    "ranked_documents_inverted = get_ranked_documents(preprocessed_tokens, \"inverted\", \"programming language\")\n",
    "ranked_documents_tfidf = get_ranked_documents(preprocessed_tokens, \"tfidf\", \"programming language\")\n",
    "\n",
    "# Display the ranked documents\n",
    "print(\"Ranked documents (Inverted Index):\", ranked_documents_inverted)\n",
    "print(\"Ranked documents (TF-IDF):\", ranked_documents_tfidf)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
